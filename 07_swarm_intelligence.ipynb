{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"07_swarm_intelligence.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyM8F8ThlgK9tC/ZFyrn79sH"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"76pywAcovTft"},"source":["# **Swarm Intelligence**\n","Swarm intelligence is an artificial intelligence techinique based on the study of collective behaviour in decentralized self-organized systems.\n","These systems are typically populations of simple agents interacting locally with each other and the environment.\n","Even though there is **no centralized control**, these local interactions often lead to the **emergence of a global behaviour**.\n","\n","Human intelligence is the result of social interaction as well: evaluate, compare, learning from experince emulating succesfull behaviours,...\n","\n","Culture and cognition are consequences of human social choices.\n","In fact, culture emerges when individual become similar through social learning.\n","So, in order to model human intelligence, there's need to model individuals in a social context.\n","\n","#Features of a Swarm Intelligence system\n","* Set of **simple individuals** that aren't aware of the system in global view;\n","* Local communication either direct or indirect (**stigmergic**);\n","* Distribuited computation (not centralized);\n","* Robustness, achieved through **graceful degradation**;\n","* Adaptivity.\n","\n","**Stigmergy**: a form of indirect communication, when an agent modifies the environment other agents may recoil and react to this.\n","\n","**Graceful adaptation**: when part of the system is destroyed, it still mantains (partial) functionality.\n","\n","**Self-organization**: is based on:\n","* **Multiple interactions** among agents, in a probabilistic way. Can be simple agents or multi-agents systems;\n","* **Positive feedback**: reinforcement of good common behaviour and amplification of random fluctuations and structure formation;\n","* **Negative feedback**: saturation, competition and resources exhaustation.\n","\n","Many different algorithms exist:\n","* **Ant colony optimization** (ACO): positive feedback given by pheromone trails that reinforce components that contribute to the problem's solution;\n","* **Artificial bee colony** (ABC): individuals with different functions;\n","* **Particle swarm optimization** (PSO): stigmergy as communication and imitation of neighbourhoods."]},{"cell_type":"markdown","metadata":{"id":"OOIMeMXi1oyO"},"source":["# Ant colony optimization (ACO)\n","From ants observation we discover that:\n","* Ants release **pheromones** trails while walking from the nest to the food and viceversa;\n","* Ants tend to chose more likely the paths marked with higher pheromone levels (but not always, is not deterministic);\n","* Cooperative interaction leads to an emergent behaviour to find the shortest path.\n","![](https://upload.wikimedia.org/wikipedia/commons/thumb/a/af/Aco_branches.svg/1280px-Aco_branches.svg.png)\n","\n","If an ant finds food it follows the pheromones left along the path to reach it and take the food back to the nest. Shorter paths will be reinforced by pheromones further, while in longer paths pheromones will evaporate.\n","\n","The **ant colony optimization** is a *probabilistic parametrized model* where ants (agents) build solution components in an incremental way.\n","The algorithm takes stochastic steps on a fully connected graph called *construction graph* $G=(C,L)$, where:\n","* Vertexes $C$ are solution components;\n","* Arcs $L$ are connections;\n","* States are paths on $G$.\n","\n","Constraints may be represented to define what is a consistent solution.\n","\n","Example of an implementation of a TSP model in ACO: nodes of the graph $G$ are the solution to be visited (solution components), arcs are connections between cities and a solution is an Hamiltonian path in the graph. Constraint are used to avoid sub-cycles (each ant can visit a city once).\n","\n","# The algorithm\n","Connection, solution components or both have associated:\n","* Pheromone $\\tau$, that abstracts natural pheromone trails and codes long term memory of the global search process;\n","* Heuristic value $\\eta$, that represents the prior background knowledge on the problem.\n","\n","Pheromones take into account all agents and each agent also uses heuristics to search locally.\n","![](https://i.ibb.co/x5rjQCV/schema.png)\n","\n","Ants build a solution following a path on the *construction graph* $G$, a transition rule is followed to choose the next node to visit. Pheromones values are updated on the basis of the **quality** of the solution found.\n","\n","> **Ant-system algorithm**:\n","* Initialize pheromone values (if 0 it would starts go around randomly);\n","* While termination conditions aren't met, for all ants $a \\in A$ construct a solution $s_a$ based on $\\tau$ and $\\eta$;\n","* Update the pheromones (**delayed update**: we update the pheromone if we find the solution);\n","* Repeat from step 2.\n","\n","Memory is used to remind past paths. Starting from node $i$, we have to probabilistically choose the next consistent node, and that choice depends, as we said before, on:\n","* Pheromone trail $\\tau_{ij}$ (global);\n","* Heuristics $\\eta_{ij} = \\frac{1}{d_{ij}}$ (individual).\n","\n","The probability will be:\n","* $p_{ij} = \\frac {[{\\tau_{ij}}]^\\alpha[{\\eta_{ij}}]^\\beta}{\\sum_{k \\text{ feasible}}[{\\tau_{ik}}]^\\alpha[{\\eta_{ik}}]^\\beta}$ if $j$ is consistent (so I can go there with my problem);\n","* $p_{ij} = 0$ otherwise.\n","\n","Where $\\alpha$ and $\\beta$ weight the importance of the pheromone and the heuristics.\n","\n","\n","The pheromone is updated with the following rule: $\\tau_{ij} \\leftarrow (1-\\rho)\\tau_{ij} + \\sum_{k=1}^{m} \\Delta \\tau_{ij}^{k}$, where:\n","* $\\rho$ is the evaporation coefficient;\n","* $\\Delta \\tau_{ij}^{k} = \\frac {1}{L_{k}}$ if ant $k$ used the arc $(i,j)$ and $0$ otherwise;\n","* $L_{k}$ is the lenght of the path followed by ant $k$.\n","\n","The high level algorithm is:\n","> * While termination condition aren't met:\n","* `AntBasedSolutionConstruction()`;\n","* `PheromoneUpdate()`;\n","* `DaemonActions()`; \n","* End while.\n","\n","`AntBasedSolutionConstruction()`: ants move by applying a stochastic local decision policy (seen before). While moving, ants take track of the partial solutions (paths) that have been built.\n","\n","`PheromoneUpdate()`: can be of two kind:\n","* Online step-by-step pheromone update: ants update during the solution construction;\n","* Online delayed pheromone update: ants update backward on the basis of the quality of the overall solution.\n","\n","In any case evaporation is applied all the time.\n","\n","`DaemonActions()`: centralized actions to boost the algorithm (it's like cheating). Local search can be applied to each built solution to improve it.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"upY4XrWGPWmX"},"source":["# Artificial Bee Colony (ABC)\n","The bees are of three types:\n","* **Employed bees**: associated with a specific nectar source;\n","* **Onlooker bees**: observe the employed bees and choose a nectar source;\n","* **Scout bees**: discover new food sources.\n","\n","Initially, food sources are discovered by scout bees. When food is consumed and the source exhausted the employed bees in that source become scouts.\n","\n","**Food position** | **Food quantity**\n","--- | ---\n","Solution | Fitness\n","*Exploration* | *Exploitation*\n","\n","We have as many solutions as the employed bees.\n","\n","> **ABC algorithm**:\n","* `InitializationPhase()`;\n","* **Repeat**\n","* `EmployedBeePhase()`;\n","* `OnlookerBeePhase()`;\n","* `ScoutBeePhase()`;\n","* Solution $\\leftarrow$ best solution so far;\n","* **Until** a termination condition is met.\n","\n","`InitializationPhase()`: a set of food source positions are randomly selected by the bees and their nectar amount are determined. Each solution $x_m$ is composed of $n$ variables $x_{m_i}$. Each variable is subjected to a lower and upper bound and it's initialized to $x_{m_i} = \\text{LB}_i + \\text{rand}(0,1)\\bigl[\\text{UB}_i - \\text{LB}_i\\bigr]$.\n","\n","`EmployedBeePhase()`: after sharing the information about the nectar amount, every employed bee goes to the food source visited by herself at the previous cycle (since it exist in it's memory) and then chooses a new food source by means of visual information in the **neighbourhood** (local search). We use an objective function to compute fitness as follows:\n","* $\\text{ftn}(x_m)=\\frac{1}{1+\\text{obj}(x_m)}$ if $\\text{obj}(x_m)\\ge 0$\n","* $\\text{ftn}(x_m)=1+ \\lvert \\text{obj}(x_m)\\rvert$ if $\\text{obj}(x_m)< 0$\n","\n","`OnlookerBeePhase()`: onlooker bees choose a food source depending on the probability associated with that source: $p_m=\\frac{\\text{ftn}(x_m)}{\\sum_{i=1}^{N}\\text{ftn}(x_i)}$, therefore the ratio between the fitness of that specific solution and the sum of the fitness of all solutions. This is a **positive feedback**.\n","\n","`ScoutBeePhase()`: scout bees choose nectar sources randomly (a kind of restart, diversification process). Employed bees that can't improve the solution after a given number of attempts become scout and leave the food source. This is a **negative feedback**.\n","\n","Some applications of this algorithm are: training of neural networks, numerical optimization and combinatorial optimization.\n"]},{"cell_type":"markdown","metadata":{"id":"Axw8yo4oW6FW"},"source":["# Particle Swarm Optimization (PSO)\n","This algorithm it's based on the analysis of interaction mechanism between individual that compose a swarm.\n","The observation of rules that guide a bird flock show that each individual tends to:\n","* Follow neighbours;\n","* Stay in the flock;\n","* Avoid collisions.\n","\n","With these rules the model of a flock has no common objective.\n","PSO adds one: food search.\n","With a common objective each individual tends to:\n","* Move away from the group to reach the food (individualistic choice);\n","* Stay in the group (social choice).\n","\n","If more than one individual entity moves toward the food, other members will do the same. Gradually the whole flock changes direction toward the promising area: the **information propagates** to all members.\n","\n","With PSO it's possible to solve optimization problems with the following analogies:\n"," \n","* **Individuals**: tentative configuration that move and sample the solution (*exploration*);\n","* **Social interaction**: each individual agent takes advantage from other searches moving towards promising regions (best solution globally found, *exploitation*).\n","\n","Search strategy can be found as a balance between *exploration* and *exploitation*.\n","\n","**Neighbourhood**: in this algorithm it's important the concept of proximity.\n","Individuals are affected by the actions of other individuals that are closer to them (sub-grouping), imitating the most performant.\n","Individuals are parts of more sub-groups and in this way the **information spreads globally**.\n","Sub-groups aren't tied to the physical proximity of the configurations in the *parameter space*, but are apriori defined and so they may also take into account considerable shifts between individuals.\n","\n","# The algorithm\n","PSO optimizes a problem by settig a population (the swarm) of candidate solutions (the particles).\n","PSO moves the particles in the search space through simple mathematical formulas: the movement is guided by the best position found so far in search space and it's updated when better solutions are discovered.\n","* **Fitness function** to minimize $f \\colon \\mathbb{R}^n \\to \\mathbb{R}$: it takes a solution ($n$-dimensional vector) and produces a fitness value. The gradient of $f$ is unknown;\n","* **Goal**: find a solution $s$ such that $f(s) \\le f(s')$ for all $s'$ in the search space.\n","\n","There are some parameters too:\n","* $S$ is the number of particles in population. Each particle has: a position $x_i \\in \\mathbb{R}^n$ in the search space and a velocity $v_i \\in \\mathbb{R}^n$;\n","* $p_i$ is the best solution found so far by particle $i$;\n","* $g$ is the best solution found so far by the entire swarm.\n","\n","For each particle $i=1, \\dots, S$ do (initialization phase):\n","1. Initialize the particle's position with an uniformly distribuited random vector $x_i \\leftarrow \\text{U}(b_{\\text{low}},b_{\\text{up}})$;\n","1. Initialize the particle's best know position to its initial position $p_i \\leftarrow x_i$;\n","1. If $f(p_i) \\le f(g)$ (means that $p_i$ is a better solution than $g$) update the swarm's best known position $g \\leftarrow p_i$;\n","1. Initialize the particle's velocity with an uniformly distribuited random vector $v_i \\leftarrow \\text{U}( -\\lvert b_{\\text{up}}-b_{\\text{low}}\\rvert,\\lvert b_{\\text{up}}-b_{\\text{low}}\\rvert)$;\n","\n","Then, until a termination criterion is met, for each particle $i=1, \\dots, S$ do (swarm moving):\n","* Pick random numbers $r_p,r_g=\\text{U}(0,1)$;\n","* Update the particle's velocity $v_i \\leftarrow \\omega v_i + \\varphi_p r_p (p_i - x_i) + \\varphi_g r_g (g-x_i)$;\n","* Update the particle's position $x_i \\leftarrow x_i + v_i$;\n","* If $f(x_i)< f(p_i)$ (means that the new solution is better than the previous), then:\n","> * Update the particle's best know position $p_i \\leftarrow x_i$;\n","  * Then if $f(p_i)< f(g)$ (means that the new solution is even better than the global one) update the swarm's best known position  $g \\leftarrow p_i$;\n","\n","At the end return the best solution found $g$.\n","\n","Parameters $\\omega$, $\\varphi_p$ and $\\varphi_g$ should be carefully selected as they strongly influence the effectiveness and efficiency of the method.\n","It's often used an automatic tuning done through iterated local search in the space of the parameters (as a proxy).\n","\n","\n","\n","\n","\n"]}]}