{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"06_local_search.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNY6h4CJNpW+pUjBe0FI6ik"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"PD3-Euy4Ynz1"},"source":["# **Local Search**\n","The **constructive algorithms** seen so far **generate** a solution by adding to a starting state solution components in a particular order.\n","Instead, **local search algorithms** start from an initial solution and **iteratively try to improve it** through local moves.\n","\n","This kind of algorithm can be useful when the **path to reach a solution isn't important** and when the state description contain all the information necessary for **understanding if one configuration is a solution or not**.\n","\n","Two key advantages: use very little memory (usually a constant amount) and can find reasonable solution in large or even infinite continuous search space.\n","\n","**Neighbourhood**: assign to every state $s\\in \\mathcal{S}$ a set of neighbours $\\mathcal{N}(s) \\subseteq \\mathcal{S}$, where $\\mathcal{N}$ is the neighbourhood of $s$, such that $\\mathcal{N}\\colon \\mathcal{S} \\to 2^{\\mathcal{S}}$\n","\n","**Examples**: \n","* **$n$-queens problem**: put $n$ queens on a chessboard so that they do not attach each other. We start from a solution, then we count the violations to move toward other solutions;\n","* **Traveling salesman problem**: given an unidirected graph, with $n$ nodes and each arc associated with a positive value, find the Hamiltonian tour (cover every city just once) with the minimum total cost.\n","\n","**$k$-exchange**: solutions are represented as a perumation of numbers, we take a solution, remove $k$ arcs and add another $k$. Candidate solutions $s$ and $s'$ are neighbours iff $s$ differs from $s'$ in at most $k$ solution components.\n","\n","**State-space landscape**: a landscape has both *location* (defined by the state) and *elevation* (define by the heuristic).\n","A **complete** local search always finds a goal if one exist and an **optimal** one always finds a global maximum/minimum.\n","![](https://www.researchgate.net/profile/Jean_Pierre_Batault/publication/337347748/figure/fig3/AS:826932534972442@1574167488631/state-space-landscape-Lazebnik-2018.ppm)\n","\n","**Local optima**: a local maximum is a solution $s$ such that for any $s'$ belonging to $\\mathcal{N}(s)$, given an evaluation function $f$ it holds:\n","* $f(s) \\ge f(s')$\n","\n","When we solve for a maximization problem we look for a **global maximum** $s_\\text{opt}$, such that for any $s$:\n","* $f(s_\\text{opt}) \\ge f(s)$\n","\n","The larger the neighbourhood the more likely a local maximum is a global one, but the large is the computational expense as well."]},{"cell_type":"markdown","metadata":{"id":"zrcjHZ7ojHXi"},"source":["# Hill-climbing search (iterative improvement)\n","It's a very basic local search algorithm that performs a move only if the solution it produces is better than the current solution, without thinking where to go next (greedy algorithm).\n","> **Hill-climbing search:** \n","1. $s$ <- generate the initial solution;\n","1. $s$ <- put the best state $s$ of the neighbourhood $\\mathcal{N}(s)$;\n","1. Repeat from step 2 until there's no improvement.\n","\n","Hill climbing often makes rapid progress toward a solution since it's easy to improve a bad state.\n","The algorithm stops as soon as it finds: \n","* **Local optima**: because there is no improvement point in all the neighbourhood;\n","* **Ridges**: sequences of local optima that is difficult for the greedy algorithm to navigate;\n","* **Plateaux**: flat areas or shoulders, so they have the same heuristic's value. The greedy algorithm may get lost in here.\n","\n","Another drawback is that the algorithm doesn't have memory of the previous states:\n","> \"It's like climbing the Everest in thick fog with amnesia\"\n"]},{"cell_type":"markdown","metadata":{"id":"RoO7cS0yrh9N"},"source":["# Meta heuristics\n","Local search alone has many pittfals, so we need more effective and general strategies, for example:\n","* Accept non-improving moves (**sideways moves**);\n","* Change neighbourhood or cost function during the search (**restart**);\n","* Using high-level search strategies called meta-heuristics.\n","\n","Local search can be seen as a search process over a graph:\n","* Search starts from an initial node and explores the graph moving in it's neighbourhood until it reach a **termination condition**;\n","* A **neighbourhood graph** to represent search space topology;\n","* A **search graph** to represent search space exploration."]},{"cell_type":"markdown","metadata":{"id":"kaK1TjVpuQYJ"},"source":["# Simulated annealing\n","It's an algorithm originated in the statistical mechanics to model the heating and cooling of metals.\n","It allows worsening moves and the probability of doing such moves is decreased during the search with a probabily $p(T,s',s)=\\exp({\\frac{-f(s')-f(s)}{T}})$\n","\n","> **Simulated annealing**:\n","1. $s$ $\\leftarrow$ generate the initial solution;\n","1. $T$ $\\leftarrow$ generate the initial temperature;\n","1. Repeat, $s'$ $\\leftarrow$ put a random solution from $\\mathcal{N}(s)$;\n","1. If $f(s') < f(s)$ then replace $s$ with $s'$;\n","1. Else accept $s'$ as a new solution with probability $p(T,s',s)$\n","1. If `update(T)` end the cycle.\n","\n","There are different ways to vary the temperature:\n","* **Logarithmic**: $T_{k+1}=\\frac{\\Gamma}{\\log(k+k_0)}$, the algorithm is guaranteed to converge to the optimal solution but it's too slow for application (exponential time complexity);\n","* **Geometric**: $T_{k+1}=\\alpha T_k$ with $\\alpha \\in (0,1)$;\n","* **Non-monotonic**: the temperature is decreased (**intensification**) and then increased again (**diversification**).\n","\n","We call **intensification** the search in the neighbourhood and **diversification** the departure from the neighbourhood (restart)."]},{"cell_type":"markdown","metadata":{"id":"s2drt2O21M1t"},"source":["# Tabu search\n","The tabu search explicitly exploits the search history to dynamically change the neighbourhood to explore.\n","\n","**Tabu list**: keeps track of recent visited solutions or moves and forbids them to escape from local minima and no cycling.\n","\n",">**Tabu search**:\n","1. $s$ $\\leftarrow$ generate the initial solution;\n","1. Tabu list $\\leftarrow$ $\\emptyset$;\n","1. $s$ $\\leftarrow$ the best of $\\mathcal{N}(s)$ without the tabu-list;\n","1. Update the tabu list;\n","1. Return to step 3 if the termination condition aren't met.\n","\n","Store a list of solution can be inefficient, therefore moves can be stored, but storing moves could cut good not yet visited solutions. We use then **aspiration criteria**, that permit to accept forbidden moves toward a solution better than the current one."]},{"cell_type":"markdown","metadata":{"id":"ZZZELCWU5rnL"},"source":["# Iterated local search\n","This algorithm uses two types of sls steps:\n","* **Subsidiary local search steps**: for reaching local optima as efficiently as possible (*intensification*);\n","* **Perturbation steps**: for effectively escaping from local optima (*diversification*).\n","\n","There's also an **acceptance criterion** to control the tradeoff between diversification and intesification.\n","\n","> **Iterated local search**:\n","1. $s_0$ $\\leftarrow$ generate the initial solution;\n","1. $s_*$ $\\leftarrow$ local search on $s_0$;\n","1. While *termination conditions aren't met*: perturbate $s_*$ to obtain a new solution $s'$, apply a local search on $s'$ to obtain $s'_*$ and then apply the acceptance criterion to compare $s_*$ and $s'_*$ (**perturbe and improve**).\n","\n","**Termination condition**: moreover being an optima, timeout, memoryout,..."]},{"cell_type":"markdown","metadata":{"id":"BDUY7_ekQd2_"},"source":["# Genetic algorithms\n","Genetic algorithms are a set of algorithms based on evolutionary biology.\n","The basic principle is learning correlations between *good* solution components.\n","These algorithms are based on the **evolution** of a set of points (**population**, rather than a single solution) in the search space.\n","From biological evolution we inherit three key principles:\n","* **Adaptation**: organisms are suited to their habitats;\n","* **Inheritance**: offspring resebles to their parents;\n","* **Natural selection**: new adapted organisms emerges, those that fail are dulled to extinction.\n","\n","The **fittest** individuals have higher chances to have numerous offspring.\n","Children are similar, but not equal to their parents.\n","So, the traits of the fittest individual spread across the population, generation by generation.\n","\n","**Biological evolution** | **Artificial systems**\n","--- | ---\n","Individual | A possible solution\n","Fitness | Quality\n","Environment | Problem\n","\n","The *evolutionary cycle*:\n","* **Recombination**: combines genetic material of parents. The goal is combining the parts from good solutions but might be achieved the opposite effect;\n","* **Mutation**: introduces variability (diversity) in genotypes;\n","* **Selection**: choice of parents whose genetic material is reproduced with variations, drives toward high fitness;\n","* **Replacement/insertion**: defines the new population.\n","![](https://slideplayer.com/slide/12965144/79/images/16/The+Evolutionary+Cycle.jpg)\n","\n","# Genetic operators\n","A population is a set of individuals or genotypes (solutions).\n","Chromosomes are made of units called genes, the domain of values of a gene is composed of alleles (mostly binary).\n","* **Recombination** (also called crossover): cross-combination of two chromosomes;\n","![](https://i.ibb.co/yS3V8Qf/Cattura.png)\n","* **Mutation**: each gene has $p_M$ probability of being *flipped*;\n","![](https://i.ibb.co/8Kfq8Vz/Cattura2.png)\n","* **Proportional selection**: probability of an indiviudal to be chosen is proportional to its fitness (roulette wheel for representation);\n","* **Generational replacement**: the new generation replace entirely the old one. Simple, cheap computationally and easier anaylsis but good solutions might be discarded (alternatively we can keep the best $n$ from the old population).\n","\n","From the analytical point of view:\n","> In terms of real valued variables:\n","  * Solution: $x \\in [a,b]$ with $a,b \\in \\mathbb{R}$;\n","  * Mutation: random perturbation $x \\to x \\pm \\sigma$, accepted if $x \\pm \\sigma \\in [a,b]$;\n","  * Crossover: linear combination $z=\\lambda_{1}x+\\lambda_{2}y$ with $\\lambda_1,\\lambda_2$ such that $a \\le z \\le b$.\n","\n","> In terms of permuation:\n","  * Solution: $x=(x_1,x_2,\\dots,x_n)$ is a permutation of $(1,2,\\dots,n)$;\n","  * Mutation: random exchange of two elements in the $n$-ple;\n","  * Crossover: like the $2$-point crossover but avoiding value repetition.\n","\n","A very high-level scheme can be:\n","> **Genetic algorithm**:\n","  1. Initialize population;\n","  1. Evaluate population;\n","  1. While termination condition aren't met;\n","  1. While new population isn't completed;\n","  1. Select two parent for mating, apply crossover, apply mutation to each new individual;\n","  1. Repeat from step 4;\n","  1. Population $\\leftarrow$ New population;\n","  1. Evaluate population;\n","  1. Return to step 3.\n","\n","Termination condition can be:\n","* Execution time limit reached;\n","* Satisfactory solutions obtained;\n","* Stagnation: population converged to the same individual.\n","\n","**Pro** | **Cons**\n","--- | ---\n","Extremely simple | Too simple operators\n","General purpose | Coding is crucial\n","Tractable theoretical models "]},{"cell_type":"markdown","metadata":{"id":"opjEsWZZ-ggr"},"source":["# Design guidelines\n","Local search and metaheuristics are preferable when:\n","* Neighbourhood structures create correlated search graph;\n","* Computational cost of the moves is low;\n","* Inventing new moves is easy.\n","\n","Population-based algorithms are preferable when:\n","* Solution can be encoded as compositions of good building blocks;\n","* Computational cost of the moves is high;\n","* It's difficult to design a neighbourhood structure;\n","* Coarse-grainde exploration is preferable as in case of huge search spaces."]}]}